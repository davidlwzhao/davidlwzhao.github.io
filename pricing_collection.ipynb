{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process blog post:\n",
    "\n",
    "Like so many others out there this quarantine has instilled a certain level of wanderlust in me. And obviously before I commit to any location I wanted to know that I'm not getting scalped on the airfare. To put my (and possibly your) mind at ease I've looked at historical airfare prices across X popular routes to get a view on the price fluctiations you can expect if you're travelling.\n",
    "\n",
    "## Table of Contents:\n",
    "0. Exec summary\n",
    "1. Data collection\n",
    "    - Schema and collection method\n",
    "    - Sources\n",
    "        - Airports & Routes\n",
    "        - Flights & Prices\n",
    "2. Data preparation and feature engineering\n",
    "    - TBC\n",
    "3. Model generation\n",
    "4. Model validation\n",
    "5. Output and visualizations\n",
    "6. Extensions\n",
    "\n",
    "## To do:\n",
    "Updates for streamlining:\n",
    "- collect regional mapping table of ICAO codes (e.g first letter K = USA)\n",
    "- collect airline IATA codes\n",
    "- possible additional data collection\n",
    "    - aircraft codes\n",
    "    - public aviation registers (# planes and type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-structuring airport and supplements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import sqlite3\n",
    "from itertools import combinations_with_replacement\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open all the csv files\n",
    "airports = pd.read_csv(\"data/airports.csv\")\n",
    "destinations = pd.read_csv(\"data/destinations.csv\")\n",
    "basics = pd.read_csv(\"data/basics.csv\")\n",
    "frequency = pd.read_csv(\"data/frequency.csv\")\n",
    "runways = pd.read_csv(\"data/runway.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicated URLS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airport</th>\n",
       "      <th>Type</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>IATA</th>\n",
       "      <th>ICAO</th>\n",
       "      <th>FAA</th>\n",
       "      <th>URL</th>\n",
       "      <th>check_country_comparison</th>\n",
       "      <th>check_url_country</th>\n",
       "      <th>check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Airport, Type, City, Country, IATA, ICAO, FAA, URL, check_country_comparison, check_url_country, check]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data integrity checks\n",
    "airports[\"check_country_comparison\"] = airports[\"Country\"].str.replace(\" \", \"-\").str.lower()\n",
    "airports[\"check_url_country\"] = airports[\"URL\"].str.split(\"/\").str[1]\n",
    "airports[\"check\"] = airports[\"check_country_comparison\"] != airports[\"check_url_country\"]\n",
    "\n",
    "# discrepancies to check\n",
    "airports[airports[\"check\"]].groupby([\"check_url_country\", \"check_country_comparison\"]).count()\n",
    "\n",
    "# duplicate URLs\n",
    "print(f\"There are { sum(airports.duplicated('URL')) } duplicated URLS\")\n",
    "airports[airports.duplicated(\"URL\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join key\n",
    "join_key = [\"Country\", \"Airport\", \"City\"]\n",
    "id_codes = [\"IATA\", \"ICAO\", \"FAA\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag duplicate by unique key\n",
    "basics_dups = basics.groupby(join_key).count()[basics.groupby(join_key).count()[\"Metric\"]>6]\n",
    "airports[\"flag_duplicate_key\"] = airports.duplicated(join_key)\n",
    "\n",
    "# flag duplicate id codes in airports\n",
    "airports[\"flag_duplicate_anycode\"] = False\n",
    "for code in id_codes:\n",
    "    airports[f\"flag_duplicate_{code}\"] = airports.duplicated(code) & airports[code].notna()\n",
    "    airports[\"flag_duplicate_anycode\"] = airports[\"flag_duplicate_anycode\"] | airports[f\"flag_duplicate_{code}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IATA 1402 0 0\n",
      "ICAO 9254 0 0\n",
      "FAA 10174 0 0\n"
     ]
    }
   ],
   "source": [
    "# basics joins into main table to add long/lat/timezone\n",
    "basics_t = (basics\n",
    "            .drop_duplicates(subset=join_key+[\"Metric\"], keep=\"first\") # need to find better solution to this\n",
    "            .set_index(join_key + [\"Metric\"])['Value']\n",
    "            .unstack()\n",
    "            .reset_index()\n",
    "           )\n",
    "\n",
    "airports_to_merge = airports[(airports[\"flag_duplicate_key\"]==False) & (airports[\"flag_duplicate_anycode\"]==False)]\n",
    "master_airports = pd.merge(airports_to_merge, basics_t, how=\"left\", on=join_key, suffixes=[\"\", \"\"])\n",
    "\n",
    "# check IATA, ICAO, FAA codes are the same\n",
    "to_drop = list(master_airports.columns[master_airports.columns.str[:5] == \"check\"])\n",
    "master_airports.loc[master_airports[\"ICAO Code\"]==\"\\n\", \"ICAO Code\"] = np.nan\n",
    "\n",
    "for code in id_codes:\n",
    "    master_airports[f\"flag_{code}_discrepancy\"] = master_airports[code].fillna(\"N/A\") != master_airports[f\"{code} Code\"].fillna(\"N/A\")\n",
    "    master_airports[f\"{code}_master\"] = np.where(master_airports[code].isna(), master_airports[f\"{code} Code\"], master_airports[code])\n",
    "    \n",
    "    print(code, \n",
    "          sum(master_airports[f\"flag_{code}_discrepancy\"]), # number discrepancies\n",
    "          sum((master_airports[f\"flag_{code}_discrepancy\"]) & (master_airports[f\"{code} Code\"].notna())), # of which are due to nan\n",
    "          sum(master_airports[f\"{code}_master\"].isna()) - sum(master_airports[code].isna()) # gaps filled\n",
    "         ) \n",
    "    \n",
    "    to_drop.extend([f\"flag_{code}_discrepancy\", f\"{code} Code\", code])\n",
    "    \n",
    "# drop extra columns\n",
    "master_airports.drop(columns = to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_counts = destinations.groupby(join_key).size().reset_index(name='nRoutes')\n",
    "master_airports = pd.merge(master_airports, \n",
    "                           dest_counts, \n",
    "                           how=\"left\", \n",
    "                           on=join_key)\n",
    "\n",
    "# theres a fair number of airports without routes. Imagine this is due to being small so no regular commerical flights\n",
    "# implication there is that there are charter flights or flights run by small operators going to these airports (OOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the routes table\n",
    "airports_selected = (master_airports[join_key + [code + \"_master\" for code in id_codes]]\n",
    "                     .rename(columns={code + \"_master\": code + \"_Source\" for code in id_codes}))\n",
    "routes = pd.merge(destinations, \n",
    "                  airports_selected, \n",
    "                  how=\"left\", \n",
    "                  on=join_key, \n",
    "                  suffixes=[\"\", \"_Source\"])\n",
    "\n",
    "airports_selected = (airports_selected[airports_selected[\"IATA_Source\"].notna()]\n",
    "                   .rename(columns={code + \"_Source\": code for code in id_codes}))\n",
    "routes = pd.merge(routes, \n",
    "                  airports_selected.rename(columns={code + \"_Source\": code for code in id_codes}),\n",
    "                  how=\"left\", \n",
    "                  left_on=[\"IATA\"], # remember to change later to add city\n",
    "                  right_on=[\"IATA\"], \n",
    "                  suffixes=[\"\", \"_Dest\"])\n",
    "\n",
    "routes.rename(columns={code: code + \"_Dest\" for code in id_codes}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32308"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how many routes can realistically look at prices for\n",
    "len(routes[(routes[\"IATA_Source\"].notna()) & (routes[\"IATA_Dest\"].notna())].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skyscanner flights\n",
    "\n",
    "Purpose of this section is to explore the data structure of the Skyscanner API and create an ETL pipeline that will periodically (daily) add historical prices to an SQLlite DB.\n",
    "\n",
    "Taking the airports and routes gather from the previous section, I now build up an updating view of the prices for those routes by airline. The value of the time series is to be able to look at how the time-till-flight (TTF) affects the price of the ticket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the Skyscanner code for each airport\n",
    "\n",
    "unique_key = [\"Country\"]\n",
    "airport_locations = airports.drop_duplicates(subset = unique_key).loc[13889:, unique_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in file parameters\n",
    "\n",
    "with open('param.json') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "headers = params[\"headers\"]\n",
    "url = params[\"url\"]\n",
    "delay = params[\"delay\"]\n",
    "long_delay = params[\"long_delay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all country codes in skyscanner\n",
    "\n",
    "results = []\n",
    "for country in airport_locations.values:\n",
    "    querystring = {\"query\": country}\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "    time.sleep(delay)\n",
    "    try:\n",
    "        res_df = pd.DataFrame.from_dict(json.loads(response.text)[\"Places\"])\n",
    "        results.append(res_df)\n",
    "    except KeyError:\n",
    "        print(country)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine into a single df and write to disk\n",
    "skyscanner_locs = pd.concat(results, axis=0, ignore_index=True)\n",
    "skyscanner_locs[\"Flag_Airport\"] = skyscanner_locs[\"CityId\"] == \"-sky\"\n",
    "skyscanner_locs.drop_duplicates(inplace=True)\n",
    "skyscanner_locs.to_csv(\"skyscanner_places.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dests = (skyscanner_locs\n",
    ".loc[(skyscanner_locs[\"Flag_Airport\"]) & (skyscanner_locs[\"PlaceName\"]!=\"United Kingdom\"), \"PlaceId\"]\n",
    ".values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HALTED\n",
      "HALTED\n",
      "HALTED\n"
     ]
    }
   ],
   "source": [
    "# search parameters\n",
    "# need to dynamically make this X days out from current day i.e. always be collecting data for next X days flights\n",
    "\n",
    "uk_code = \"UK-sky\"\n",
    "date_outbound = \"2020-10-20\"\n",
    "date_inbound = \"2020-11-20\"\n",
    "dates = [1]\n",
    "combos = combinations_with_replacement(skyscanner_locs.loc[skyscanner_locs[\"Flag_Airport\"], \"PlaceId\"].values, 1)\n",
    "collected = defaultdict(list)\n",
    "start_time = time.time()\n",
    "counter = 0\n",
    "rerun = []\n",
    "\n",
    "# for dates in range:\n",
    "for date in dates:\n",
    "    for dest in combos:\n",
    "        dest = dest[0]\n",
    "        if dest == uk_code:\n",
    "            continue\n",
    "            \n",
    "        # flexible delay to allow for max per min   \n",
    "        elapsed_time = time.time() - start_time\n",
    "        if counter >= 50 and elapsed_time <= 60:\n",
    "            time.sleep(61-elapsed_time)\n",
    "            start_time = time.time()\n",
    "            counter = 0\n",
    "        \n",
    "        # make API call\n",
    "        price_res = make_call(uk_code, dest, date_outbound, date_inbound, headers)\n",
    "        counter = counter + 1\n",
    "        \n",
    "        # check result valid\n",
    "        if \"ValidationErrors\" in price_res:\n",
    "            continue\n",
    "        \n",
    "        if \"message\" in price_res:\n",
    "            print(\"HALTED\")\n",
    "            time.sleep(long_delay)\n",
    "            rerun.append((dest, date))\n",
    "            continue\n",
    "            \n",
    "        for k, v in flatten_json(price_res).items():\n",
    "            collected[k].append(v)\n",
    "            \n",
    "        time.sleep(0.25)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing collected data\n",
    "col_mapping = {\n",
    "    \"Routes\": \"QuoteIds\",\n",
    "    \"Quotes\": \"CarrierIds\"\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "for k, v in collected.items():\n",
    "    \n",
    "    df = pd.concat(v, axis=0, ignore_index=True)\n",
    "    \n",
    "    try:\n",
    "        col = col_mapping[k]\n",
    "        df[col] = df[col].apply(lambda x: [] if not isinstance(x, list) else x)\n",
    "        df = explode(df, [col], fill_value=\"\")\n",
    "    except KeyError:\n",
    "        continue\n",
    "    finally:\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # add_to_db(df, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CarrierId</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50441</td>\n",
       "      <td>easyJet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>881</td>\n",
       "      <td>British Airways</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1878</td>\n",
       "      <td>Wizz Air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>840</td>\n",
       "      <td>Air Algerie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1090</td>\n",
       "      <td>Ryanair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>1090</td>\n",
       "      <td>Ryanair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1755</td>\n",
       "      <td>Turkish Airlines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1090</td>\n",
       "      <td>Ryanair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1324</td>\n",
       "      <td>KLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1090</td>\n",
       "      <td>Ryanair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CarrierId              Name\n",
       "0        50441           easyJet\n",
       "1          881   British Airways\n",
       "2         1878          Wizz Air\n",
       "3          840       Air Algerie\n",
       "4         1090           Ryanair\n",
       "..         ...               ...\n",
       "260       1090           Ryanair\n",
       "261       1755  Turkish Airlines\n",
       "262       1090           Ryanair\n",
       "263       1324               KLM\n",
       "264       1090           Ryanair\n",
       "\n",
       "[265 rows x 2 columns]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_db(df, k):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   QuoteId  MinPrice  Direct        QuoteDateTime CarrierIds  OriginId  \\\n",
       " 0        1      67.0    True  2020-09-25T09:08:00     [1665]     63100   \n",
       " 1        2      78.0    True  2020-09-23T18:21:00     [1665]     68866   \n",
       " 2        3      61.0    True  2020-09-24T03:03:00     [1665]     63100   \n",
       " \n",
       "    DestinationId        DepartureDate  \n",
       " 0          56321  2020-10-01T00:00:00  \n",
       " 1          63100  2020-10-01T00:00:00  \n",
       " 2          63192  2020-10-01T00:00:00  ]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected[\"Quotes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   OriginId  DestinationId QuoteIds  Price        QuoteDateTime\n",
       " 0       838          43188      NaN    NaN                  NaN\n",
       " 1       838          51572      NaN    NaN                  NaN\n",
       " 2       838          56321      [1]   67.0  2020-09-25T09:08:00\n",
       " 3       838          63100      [2]   78.0  2020-09-23T18:21:00\n",
       " 4       838          63192      [3]   61.0  2020-09-24T03:03:00\n",
       " 5       838          68866      NaN    NaN                  NaN\n",
       " 6       838          97610      NaN    NaN                  NaN]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected[\"Routes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   CarrierId     Name\n",
       " 0       1665  Kam Air]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected[\"Carriers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the routes have been structured for mass API quering, I iterate through the combinations to find the day's flight prices. \n",
    "\n",
    "N.B. Automate this to happen every day at X time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIManager:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def create_url(origin, dest, date_outbound, date_inbound, country, currency, locale):\n",
    "    return f\"https://skyscanner-skyscanner-flight-search-v1.p.rapidapi.com/apiservices/browseroutes/v1.0/{country}/{currency}/{locale}/{origin}/{dest}/{date_outbound}\"\n",
    "\n",
    "\n",
    "def make_call(origin, dest, date_outbound, date_inbound, headers, country = \"UK\", currency = \"GBP\", locale = \"en-UK\"):\n",
    "    url = create_url(origin, dest, date_outbound, date_inbound, country, currency, locale)\n",
    "    querystring = {\"inboundpartialdate\":date_inbound}\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "    response_json = json.loads(response.text)\n",
    "    # print(json.dumps(response_json, indent=2))\n",
    "    return response_json\n",
    "\n",
    "\n",
    "def flatten_json(quotes):\n",
    "    for i, _ in enumerate(quotes['Quotes']):\n",
    "        \n",
    "        for key, val in quotes['Quotes'][i]['OutboundLeg'].items():\n",
    "            quotes['Quotes'][i][key] = val\n",
    "            \n",
    "        del(quotes['Quotes'][i]['OutboundLeg'])\n",
    "        \n",
    "    # json to DataFrames\n",
    "    # for each then need to enforce data types as well as rename any relevant columns \n",
    "    collected = {\n",
    "        \"Carriers\" : pd.DataFrame.from_dict(quotes['Carriers']),\n",
    "        \"Places\" : pd.DataFrame.from_dict(quotes['Places']),\n",
    "        \"Quotes\" : pd.DataFrame.from_dict(quotes['Quotes']),\n",
    "        \"Routes\" : pd.DataFrame.from_dict(quotes['Routes'])  \n",
    "    }\n",
    "    return collected\n",
    "\n",
    "def explode(df, lst_cols, fill_value='', preserve_index=False):\n",
    "    # make sure `lst_cols` is list-alike\n",
    "    if (lst_cols is not None\n",
    "        and len(lst_cols) > 0\n",
    "        and not isinstance(lst_cols, (list, tuple, np.ndarray, pd.Series))):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "    # preserve original index values    \n",
    "    idx = np.repeat(df.index.values, lens)\n",
    "    # create \"exploded\" DF\n",
    "    res = (pd.DataFrame({\n",
    "                col:np.repeat(df[col].values, lens)\n",
    "                for col in idx_cols},\n",
    "                index=idx)\n",
    "             .assign(**{col:np.concatenate(df.loc[lens>0, col].values)\n",
    "                            for col in lst_cols}))\n",
    "    # append those rows that have empty lists\n",
    "    if (lens == 0).any():\n",
    "        # at least one list in cells is empty\n",
    "        res = (res.append(df.loc[lens==0, idx_cols], sort=False)\n",
    "                  .fillna(fill_value))\n",
    "    # revert the original index order\n",
    "    res = res.sort_index()\n",
    "    # reset index if requested\n",
    "    if not preserve_index:        \n",
    "        res = res.reset_index(drop=True)\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
