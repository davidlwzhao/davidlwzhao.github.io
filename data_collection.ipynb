{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process blog post:\n",
    "\n",
    "Like so many others out there this quarantine has instilled a certain level of wanderlust in me. And obviously before I commit to any location I wanted to know that I'm not getting scalped on the airfare. To put my (and possibly your) mind at ease I've looked at historical airfare prices across X popular routes to get a view on the price fluctiations you can expect if you're travelling.\n",
    "\n",
    "## Table of Contents:\n",
    "0. Exec summary\n",
    "1. Data collection\n",
    "    - Schema and collection method\n",
    "    - Sources\n",
    "        - Airports & Routes\n",
    "        - Flights & Prices\n",
    "2. Data preparation and feature engineering\n",
    "    - TBC\n",
    "3. Model generation\n",
    "4. Model validation\n",
    "5. Output and visualizations\n",
    "6. Extensions\n",
    "\n",
    "## To do:\n",
    "Updates for streamlining:\n",
    "- move API key to external gitignored file\n",
    "- set up gitignore\n",
    "- consolidate imports at beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "### Airports and Routes:\n",
    "Scraping a complete list of Airports with additional characteristics - long and lat for catchment analysis\n",
    "\n",
    "Source: https://www.world-airport-codes.com/alphabetical/country-name/a.html?page=30\n",
    "\n",
    "Additional things that might be interesting to do:\n",
    "- Make airport scraping update regularly and have historical view on closure/openning and other variables (i.e. new runway)\n",
    "- Add on airport volumes from other data sources\n",
    "\n",
    "#### Process:\n",
    "---------------------\n",
    "To query the population of relevant routes from Skyscanner, I first needed to collect a comprehensive (or as close to as possible) list of airports. \n",
    "1. Find all airport URLS\n",
    "    - For each letter in alphabet:\n",
    "        - For each page:\n",
    "            - For each linked airport\n",
    "                - Scrape airport hyperlink and associated data\n",
    "                \n",
    "2. Collect informaton on seperate airport URL pages\n",
    "    - For each airport URL:\n",
    "        - Collect relevant airport specific info\n",
    "        - Add to data store\n",
    "        \n",
    "3. Check data is consistent between first page and specific URLS\n",
    "\n",
    "4. Summary statistics\n",
    "\n",
    "----------------------\n",
    "All possible routes then becomes the 2-combinaton of all n routes in the collected list.\n",
    "\n",
    "N.B. interesting to look at the number of routes each airport is directly connected, what average is by country, airport size, etc\n",
    "N.B. interesting to see number of actual routes as % of all possible. Imagine <1% given factorial nature of combinations..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airports List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the URLs of each airport on world-airport-codes.com\n",
    "from string import ascii_lowercase\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from datatime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download chromedriver \n",
    "# allow use with - https://stackoverflow.com/questions/60362018/macos-catalinav-10-15-3-error-chromedriver-cannot-be-opened-because-the-de\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(15)\n",
    "\n",
    "# set up pages to scrape\n",
    "BASE_URL = \"https://www.world-airport-codes.com/alphabetical/country-name/{letter}.html?page={page_n}\"\n",
    "starting_urls = [[BASE_URL.format(letter=letter, page_n=n) for n in range(1, 500)] for letter in ascii_lowercase]\n",
    "\n",
    "all_tables = []\n",
    "for group_of_urls in starting_urls:\n",
    "    for url in group_of_urls:\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html\")\n",
    "        tables = soup.find_all(\"table\")\n",
    "        if len(tables) == 0:\n",
    "            break\n",
    "        all_tables.append(tables)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect raw tables in dataframes for later processing\n",
    "raw_dataframes = []\n",
    "\n",
    "for table_group in all_tables:\n",
    "    for table in table_group:\n",
    "        text_data = [[cell.text for cell in row.find_all([\"th\",\"td\"])] \n",
    "             + [cell.get(\"href\") for cell in row.find_all([\"a\"])]\n",
    "                        for row in table.find_all(\"tr\")]\n",
    "        raw_dataframes.append(pd.DataFrame(text_data))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending all tables into one\n",
    "airports = pd.concat(raw_dataframes, axis=0, ignore_index=True)\n",
    "\n",
    "# naming fields\n",
    "airports.columns = list(airports.iloc[0,:-1].values) + [\"URL\"]\n",
    "\n",
    "# drop extra header rows\n",
    "airports.drop(airports[airports[\"Airport\"] == \"Airport\"].index, inplace=True)\n",
    "\n",
    "# removing whitespace and column names from raw text\n",
    "for column in airports.columns:\n",
    "    airports[column] = airports[column].map(lambda text: text.replace(f\"{column}:\", \"\").strip())\n",
    "\n",
    "# removing \"Closed\" tag from airport name\n",
    "airports.loc[airports[\"Type\"] == \"Closed\", \"Airport\"] = airports.loc[airports[\"Type\"] == \"Closed\", \"Airport\"].str[:-7]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk\n",
    "airports.to_csv(\"airports.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping additional airport characteristics from unique webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base URL\n",
    "airport_full_urls_base = \"https://www.world-airport-codes.com\"\n",
    "\n",
    "# filter out dummy urls and get inputs\n",
    "airports = pd.read_csv(\"airports.csv\")\n",
    "airports[\"URL\"] = airport_full_urls_base + airports[\"URL\"]\n",
    "to_search =  airports.loc[airports[\"URL\"].notna(), [\"Airport\",\"URL\",\"Country\", \"City\"]]\n",
    "urls = list(to_search[\"URL\"].values)\n",
    "tags = to_search[[\"Airport\",\"Country\", \"City\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOP version of individual page scraping\n",
    "start = 0\n",
    "\n",
    "manager = ScrapeManager(urls, tags, start)\n",
    "supplements = manager.scrape()\n",
    "manager.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeManager:\n",
    "    def __init__(self, urls, tags, start_i=0):\n",
    "        self._start_new_driver()\n",
    "        self.final_i = len(urls)\n",
    "        self.start_i = start_i\n",
    "        self.urls = urls\n",
    "        self.tags = tags\n",
    "        self.supplements = defaultdict(list)\n",
    "        \n",
    "    def scrape(self):\n",
    "        i = self.start_i\n",
    "        while i < self.final_i:\n",
    "            try:                \n",
    "                # get page source of new webpage\n",
    "                self.driver.get(self.urls[i])\n",
    "                element = self.wait.until(EC.presence_of_element_located((By.ID, \"metar-observations-title\")))\n",
    "                soup = BeautifulSoup(self.driver.page_source, \"html\")\n",
    "\n",
    "                # instantiate and task scraper\n",
    "                scraper = AirportScraper(soup, *self.tags[i])\n",
    "                raw_supplement = scraper.scrape()\n",
    "                \n",
    "                # add data to storage dict\n",
    "                for k, v in raw_supplement.items():\n",
    "                    self.supplements[k].append(v)\n",
    "                    \n",
    "            except TimeoutException:\n",
    "                # restart driver and try again\n",
    "                print(f\"Timed out on {i}\")\n",
    "                self._start_new_driver()\n",
    "                i = i - 1\n",
    "                \n",
    "            except WebDriverException:\n",
    "                # restart driver and try again\n",
    "                print(f\"Webdriver failed on {i}\")\n",
    "                self._start_new_driver()\n",
    "                i = i - 1\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(f\"Latest i = {i-1}: {self.urls[i-1]}\")\n",
    "                print(f\"Restart with i = {i}: {self.urls[i]}\\n\\n\")\n",
    "                self._backup_progress() # save to csv\n",
    "                sys.exit()\n",
    "                \n",
    "            i += 1\n",
    "\n",
    "        return self._backup_progress()\n",
    "    \n",
    "    def _backup_progress(self):\n",
    "        # consolidate seperate dfs then save down\n",
    "        combined = {}\n",
    "\n",
    "        for k, v in self.supplements.items():\n",
    "            combined[k] = pd.concat(v, ignore_index=True)\n",
    "\n",
    "            # rename columns\n",
    "            if k != \"Basics\":\n",
    "                combined[k].columns = list(combined[k].iloc[0,:-3].values)+[\"Airport\", \"Country\", \"City\"]\n",
    "\n",
    "            # filter out junk rows\n",
    "            first_column = combined[k].columns[0]\n",
    "            combined[k].drop(combined[k][combined[k][first_column] == first_column].index, inplace=True)\n",
    "            \n",
    "        # write supplements to disk\n",
    "        timecode = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        for supp_type, data in combined.items():\n",
    "            data.to_csv(f\"discrete/data/{supp_type.lower()}_{timecode}.csv\", index=False, encoding='utf-8-sig')\n",
    "            \n",
    "        return combined\n",
    "            \n",
    "    def _start_new_driver(self):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.wait = WebDriverWait(self.driver, 60)\n",
    "        \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "\n",
    "class AirportScraper: \n",
    "    def __init__(self, soup, airport, country, city):\n",
    "        self.raw_supplements = {}\n",
    "        self.soup = soup\n",
    "        self.airport = airport # can refactor this into a dict\n",
    "        self.country = country\n",
    "        self.city = city\n",
    "        self.scraped = []\n",
    "        \n",
    "    def scrape(self):\n",
    "        try:\n",
    "            self._scrape_supplements()\n",
    "        except Exception as e:\n",
    "            print(\"ERROR:\", e.__class__, \"occurred.\", self.airport, self.country)\n",
    "            raise RuntimeError(\"Something wrong went wrong\")\n",
    "        \n",
    "        try:\n",
    "            self._scrape_basic_info()\n",
    "        except Exception as e:\n",
    "            print(\"ERROR:\", e.__class__, \"occurred.\", self.airport, self.country)\n",
    "            raise RuntimeError(\"Something wrong went wrong 2\")\n",
    "        \n",
    "        if len(self.raw_supplements):\n",
    "            self._label()\n",
    "        \n",
    "        return self.raw_supplements\n",
    "    \n",
    "    def _label(self):\n",
    "        # adds in unique key as airport & country for later joins\n",
    "        for supp_type, df in self.raw_supplements.items():\n",
    "            df[\"Airport\"] = self.airport\n",
    "            df[\"Country\"] = self.country\n",
    "            df[\"City\"] = self.country\n",
    "    \n",
    "    def _classify_supplement(self, series):\n",
    "        \"\"\"\n",
    "        Classifies the supplementary table scraped from airport webpage\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        series : int\n",
    "            Pandas series object. List of column headers for table to classify\n",
    "        airport : str\n",
    "            Airport data relates to\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        classification: str\n",
    "\n",
    "        \"\"\"\n",
    "        mapping = {\n",
    "            \"Frequency\": [\"Type\", \"Description\", \"Frequency (MHz)\"],\n",
    "            \"Runway\": ['Runway', 'Length (feet)', 'Width (feet)', 'Surface Type'],\n",
    "            \"Destinations\": ['Destination', 'IATA', 'Airlines Flying Route'],\n",
    "            \"Helipads\": [\"Helipad\", \"Helipad Dimensions\", \"Helipad Surface Type\"]\n",
    "        }\n",
    "        \n",
    "        for k, v in mapping.items():\n",
    "            if len(series) == len(v) and all(series == v):\n",
    "                return k\n",
    "\n",
    "        # broken\n",
    "        print(series, self.airport)\n",
    "        return None\n",
    "\n",
    "    def _scrape_supplements(self):\n",
    "        tbl = self.soup.find_all(\"table\")\n",
    "\n",
    "        for table in tbl:\n",
    "            text_data = [[cell.text for cell in row.find_all([\"th\",\"td\"])]\n",
    "                                    for row in table.find_all(\"tr\")]\n",
    "\n",
    "            raw = pd.DataFrame(text_data)\n",
    "            supp_type = self._classify_supplement(raw.iloc[0])\n",
    "\n",
    "            self.raw_supplements[supp_type] = raw\n",
    "            self.scraped.append(supp_type)\n",
    "\n",
    "    def _scrape_basic_info(self):    \n",
    "        basic_info = self.soup.find_all(\"div\", class_= \"airport-basic-data\")\n",
    "        \n",
    "        if len(basic_info) == 0:\n",
    "            print(basic_info)\n",
    "            print(self.soup.find_all(\"div\"))\n",
    "\n",
    "        basics = [[cell.text for cell in row.find_all([\"strong\", \"span\"])] \n",
    "                             for row in basic_info[0].find_all(\"div\")]\n",
    "\n",
    "        basics_df = pd.DataFrame(basics, columns=[\"Metric\", \"Value\"])\n",
    "\n",
    "        self.raw_supplements[\"Basics\"] = basics_df\n",
    "        self.scraped.extend(list(basics_df[\"Metric\"].values))\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
